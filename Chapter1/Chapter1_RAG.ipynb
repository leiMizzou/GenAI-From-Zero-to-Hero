{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 2：导入必要的库\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 3：配置API密钥和基础URL\n",
    "# 请确保已经在系统环境变量中设置了 OPENAI_API_KEY 和 OPENAI_API_BASE\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"sk-16a90ba86cfc4dcf9402bea1309c9021\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\", \"https://api.deepseek.com\")\n",
    "\n",
    "# 验证API密钥是否设置\n",
    "if not openai.api_key:\n",
    "    logging.warning(\"OPENAI_API_KEY未设置。请在环境变量中设置它以启用API调用。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 4：加载嵌入模型\n",
    "\n",
    "# 加载嵌入模型\n",
    "device = torch.device(\"cpu\")  # 强制使用CPU\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
    "    logging.info(\"嵌入模型加载成功！\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"加载嵌入模型时发生错误: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 5：定义嵌入函数\n",
    "\n",
    "# 嵌入函数（批量处理）\n",
    "def embed_texts(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # 使用CLS token的输出作为句子的嵌入\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 6：加载文档数据\n",
    "\n",
    "# 加载文档数据（CSV文件）\n",
    "def load_documents(file_path, nrows=100):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=nrows)\n",
    "        df = df.dropna(subset=['text'])\n",
    "        logging.info(f\"成功加载了 {len(df)} 条文档。\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"文件 {file_path} 未找到。请检查路径是否正确。\")\n",
    "        return pd.DataFrame(columns=['text'])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"加载文档时发生错误: {e}\")\n",
    "        return pd.DataFrame(columns=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 7：构建FAISS索引\n",
    "\n",
    "# 构建FAISS索引\n",
    "def build_faiss_index(embeddings, use_quantization=False):\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    if use_quantization:\n",
    "        # 使用Product Quantization进行压缩\n",
    "        nlist = 100  # 聚类数\n",
    "        quantizer = faiss.IndexFlatL2(dimension)\n",
    "        index = faiss.IndexIVFPQ(quantizer, dimension, nlist, 16, 8)  # 16 bytes per vector, 8 subquantizers\n",
    "        index.train(embeddings)\n",
    "        index.add(embeddings)\n",
    "        logging.info(\"使用量化的FAISS索引已构建。\")\n",
    "    else:\n",
    "        # 使用简单的扁平索引（不量化）\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "        logging.info(\"使用扁平FAISS索引已构建。\")\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 8：检索相关文档\n",
    "\n",
    "# 检索相关文档\n",
    "def retrieve_relevant_documents(index, query_embedding, texts, top_k=2):\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    return [texts[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 9：生成回答\n",
    "\n",
    "# 使用DeepSeek生成回答\n",
    "def generate_response(prompt):\n",
    "    if not openai.api_key:\n",
    "        return \"API密钥未设置。请设置OPENAI_API_KEY环境变量。\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"API调用失败: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 10：主流程\n",
    "\n",
    "# 主流程\n",
    "def main(use_csv=True, file_path=\"documents.csv\", nrows=100, use_quantization=False, top_k=2):\n",
    "    try:\n",
    "        if use_csv:\n",
    "            # 使用CSV加载文档\n",
    "            logging.info(\"正在加载CSV文档...\")\n",
    "            df = load_documents(file_path, nrows=nrows)\n",
    "            \n",
    "            if df.empty:\n",
    "                logging.error(\"未加载到任何文档。请检查CSV文件。\")\n",
    "                return\n",
    "            \n",
    "            texts = df['text'].tolist()\n",
    "        else:\n",
    "            # 使用默认文档列表\n",
    "            logging.info(\"未使用CSV，使用默认文档。\")\n",
    "            texts = [\n",
    "                \"这是第一段默认的文本。\",\n",
    "                \"这是第二段默认的文本。\",\n",
    "                \"这是第三段默认的文本。\"\n",
    "            ]\n",
    "            \n",
    "            if not texts:\n",
    "                logging.error(\"默认文档列表为空。\")\n",
    "                return\n",
    "        \n",
    "        # 生成嵌入\n",
    "        logging.info(\"正在生成嵌入...\")\n",
    "        embeddings = embed_texts(texts)\n",
    "        \n",
    "        # 构建FAISS索引\n",
    "        logging.info(\"正在构建FAISS索引...\")\n",
    "        faiss_index = build_faiss_index(embeddings, use_quantization=use_quantization)\n",
    "        \n",
    "        # 获取用户输入（这里使用固定的问题）\n",
    "        user_input = \"CHIMA是谁?\"\n",
    "        \n",
    "        # 生成查询嵌入\n",
    "        logging.info(\"正在生成查询嵌入...\")\n",
    "        query_embedding = embed_texts([user_input])[0]\n",
    "        \n",
    "        # 检索相关文档\n",
    "        logging.info(\"正在检索相关文档...\")\n",
    "        relevant_docs = retrieve_relevant_documents(faiss_index, query_embedding, texts, top_k=top_k)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            logging.warning(\"未检索到相关文档。\")\n",
    "            return\n",
    "        \n",
    "        # 将检索到的文档作为上下文\n",
    "        context = \"\\n\".join(relevant_docs)\n",
    "        logging.info(f\"检索到的上下文内容如下：\\n{context}\")\n",
    "        \n",
    "        # 使用DeepSeek生成回答\n",
    "        logging.info(\"正在生成AI回答...\")\n",
    "        #prompt = f\"根据以下上下文回答用户问题：\\n\\n上下文：\\n{context}\\n\\n问题：\\n{user_input}\"\n",
    "        prompt = f\"根据以下上下文回答用户问题：\\n\\n上下文：\\n{context}\\n\\n问题：\\n{user_input}。如果缺少上下文则根据你的知识回答。\"\n",
    "        ai_response = generate_response(prompt)\n",
    "        \n",
    "        print(\"\\nAI回答：\")\n",
    "        print(ai_response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"发生错误: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 11：运行主流程\n",
    "\n",
    "# 运行主流程，使用CSV加载文档\n",
    "main(use_csv=True, file_path=\"documents.csv\", nrows=100, use_quantization=False, top_k=2)\n",
    "#main(use_csv=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
