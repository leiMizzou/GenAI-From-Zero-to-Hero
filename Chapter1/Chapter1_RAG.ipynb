{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 2：导入必要的库\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 18:56:21,851 - INFO - 嵌入模型加载成功！\n"
     ]
    }
   ],
   "source": [
    "# 步骤 4：加载嵌入模型\n",
    "\n",
    "# 加载嵌入模型\n",
    "device = torch.device(\"cpu\")  # 强制使用CPU\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
    "    logging.info(\"嵌入模型加载成功！\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"加载嵌入模型时发生错误: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 5：定义嵌入函数\n",
    "\n",
    "# 嵌入函数（批量处理）\n",
    "def embed_texts(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # 使用CLS token的输出作为句子的嵌入\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 6：加载文档数据\n",
    "\n",
    "# 加载文档数据（CSV文件）\n",
    "def load_documents(file_path, nrows=100):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=nrows)\n",
    "        df = df.dropna(subset=['text'])\n",
    "        logging.info(f\"成功加载了 {len(df)} 条文档。\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"文件 {file_path} 未找到。请检查路径是否正确。\")\n",
    "        return pd.DataFrame(columns=['text'])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"加载文档时发生错误: {e}\")\n",
    "        return pd.DataFrame(columns=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 7：构建FAISS索引\n",
    "\n",
    "# 构建FAISS索引\n",
    "def build_faiss_index(embeddings, use_quantization=False):\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    if use_quantization:\n",
    "        # 使用Product Quantization进行压缩\n",
    "        nlist = 100  # 聚类数\n",
    "        quantizer = faiss.IndexFlatL2(dimension)\n",
    "        index = faiss.IndexIVFPQ(quantizer, dimension, nlist, 16, 8)  # 16 bytes per vector, 8 subquantizers\n",
    "        index.train(embeddings)\n",
    "        index.add(embeddings)\n",
    "        logging.info(\"使用量化的FAISS索引已构建。\")\n",
    "    else:\n",
    "        # 使用简单的扁平索引（不量化）\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "        logging.info(\"使用扁平FAISS索引已构建。\")\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 8：检索相关文档\n",
    "\n",
    "# 检索相关文档\n",
    "def retrieve_relevant_documents(index, query_embedding, texts, top_k=2):\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    return [texts[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 9：生成回答\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "llm = OpenAI(\n",
    "    api_key=\"sk-16a90ba86cfc4dcf9402bea1309c9021\",\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# 使用DeepSeek生成回答\n",
    "def generate_response(prompt):\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 10：主流程\n",
    "\n",
    "# 主流程\n",
    "def main(use_csv=True, file_path=\"documents.csv\", nrows=100, use_quantization=False, top_k=2):\n",
    "    try:\n",
    "        if use_csv:\n",
    "            # 使用CSV加载文档\n",
    "            logging.info(\"正在加载CSV文档...\")\n",
    "            df = load_documents(file_path, nrows=nrows)\n",
    "            \n",
    "            if df.empty:\n",
    "                logging.error(\"未加载到任何文档。请检查CSV文件。\")\n",
    "                return\n",
    "            \n",
    "            texts = df['text'].tolist()\n",
    "        else:\n",
    "            # 使用默认文档列表\n",
    "            logging.info(\"未使用CSV，使用默认文档。\")\n",
    "            texts = [\n",
    "                \"这是第一段默认的文本。\",\n",
    "                \"这是第二段默认的文本。\",\n",
    "                \"这是第三段默认的文本。\"\n",
    "            ]\n",
    "            \n",
    "            if not texts:\n",
    "                logging.error(\"默认文档列表为空。\")\n",
    "                return\n",
    "        \n",
    "        # 生成嵌入\n",
    "        logging.info(\"正在生成嵌入...\")\n",
    "        embeddings = embed_texts(texts)\n",
    "        \n",
    "        # 构建FAISS索引\n",
    "        logging.info(\"正在构建FAISS索引...\")\n",
    "        faiss_index = build_faiss_index(embeddings, use_quantization=use_quantization)\n",
    "        \n",
    "        # 获取用户输入（这里使用固定的问题）\n",
    "        user_input = \"CHIMA是谁?\"\n",
    "        \n",
    "        # 生成查询嵌入\n",
    "        logging.info(\"正在生成查询嵌入...\")\n",
    "        query_embedding = embed_texts([user_input])[0]\n",
    "        \n",
    "        # 检索相关文档\n",
    "        logging.info(\"正在检索相关文档...\")\n",
    "        relevant_docs = retrieve_relevant_documents(faiss_index, query_embedding, texts, top_k=top_k)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            logging.warning(\"未检索到相关文档。\")\n",
    "            return\n",
    "        \n",
    "        # 将检索到的文档作为上下文\n",
    "        context = \"\\n\".join(relevant_docs)\n",
    "        logging.info(f\"检索到的上下文内容如下：\\n{context}\")\n",
    "        \n",
    "        # 使用DeepSeek生成回答\n",
    "        logging.info(\"正在生成AI回答...\")\n",
    "        #prompt = f\"根据以下上下文回答用户问题：\\n\\n上下文：\\n{context}\\n\\n问题：\\n{user_input}\"\n",
    "        prompt = f\"根据以下上下文回答用户问题：\\n\\n上下文：\\n{context}\\n\\n问题：\\n{user_input}。如果缺少上下文则根据你的知识回答。\"\n",
    "        ai_response = generate_response(prompt)\n",
    "        \n",
    "        print(\"\\nAI回答：\")\n",
    "        print(ai_response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"发生错误: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 18:56:25,454 - INFO - 正在加载CSV文档...\n",
      "2024-11-30 18:56:25,542 - INFO - 成功加载了 3 条文档。\n",
      "2024-11-30 18:56:25,548 - INFO - 正在生成嵌入...\n",
      "2024-11-30 18:56:26,053 - INFO - 正在构建FAISS索引...\n",
      "2024-11-30 18:56:26,058 - INFO - 使用扁平FAISS索引已构建。\n",
      "2024-11-30 18:56:26,059 - INFO - 正在生成查询嵌入...\n",
      "2024-11-30 18:56:26,081 - INFO - 正在检索相关文档...\n",
      "2024-11-30 18:56:26,085 - INFO - 检索到的上下文内容如下：\n",
      "中国医院协会信息专业委员会(CHIMA)，为中国医院协会所属的分支机构，是从事医院信息化的非营利性、群众性的行业学术组织。CHIMA的主要工作：开展国内外医院信息学术交流活动，制定有关医院信息标准规范及规章制度，培训和提高医院信息人员专业水平，从而推动中国医院信息化工作事业的发展。CHIMA的前身是中华医院管理学会的计算机应用学组，于1985年成立，由时任北京民航总医院院长的王志明教授和北京协和医院计算机室主任的李包罗教授任学组组长和副组长。1997年，中华医院管理学会独立成为一级学会，1998年8月，成立了中华医院管理学会信息管理专业委员会(CHIMA)，由李包罗任主任委员。2006年2月，中华医院管理学会改名为“中国医院协会”，学会亦改名为中国医院协会信息管理专业委员会。2019年换届后更名为“中国医院协会信息专业委员会”，由原国家卫生健康委统计信息中心副主任王才有担任主任委员。经过40年发展，CHIMA已经发展成为中国最大的医疗卫生信息化专业学术团体，形成覆盖医疗、卫生各个领域HIT基础和应用研究的专业组织体系，是中国HIT界最具影响力和领导力的学术组织之一。截至2023年6月换届，CHIMA委员共计422位，其中主任委员1位、副主任委员13位、秘书长1位、常务委员119位。\n",
      "北京卫生信息技术协会(PHITA)，是由北京地区从事卫生信息技术和管理的个人、医疗机构以及信息技术企业自愿组成的、实行行业服务和自律管理的非营利性社会团体。协会宗旨：团结卫生信息技术从业者，协助落实政府相关方针和政策，汇集各方资源，开展卫生信息技术领域的学术研究和交流活动，发挥行业指导、协调和监督职能，促进医疗卫生行业的信息应用与共享，推动信息化建设有序发展。本会遵守宪法、法律、法规和国家政策，践行社会主义核心价值观，弘扬爱国主义精神，遵守社会道德风尚，恪守公益宗旨，积极履行社会责任，自觉加强诚信自律建设，诚实守信，规范发展，提高社会公信力。负责人遵纪守法，勤勉尽职，保持良好个人社会信用。协会业务范围：卫生信息技术相关的政策宣传、会议及展览服务、专业培训、信息咨询、组织考察、对外交流、承办委托、编辑专刊。\n",
      "2024-11-30 18:56:26,085 - INFO - 正在生成AI回答...\n",
      "2024-11-30 18:56:26,423 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI回答：\n",
      "CHIMA是中国医院协会信息专业委员会（China Hospital Information Management Association）的简称。它是中国医院协会所属的分支机构，是一个从事医院信息化的非营利性、群众性的行业学术组织。CHIMA的主要工作包括开展国内外医院信息学术交流活动，制定有关医院信息标准规范及规章制度，培训和提高医院信息人员专业水平，从而推动中国医院信息化工作事业的发展。CHIMA的前身是中华医院管理学会的计算机应用学组，成立于1985年，经过多年的发展，已经成为中国最大的医疗卫生信息化专业学术团体之一。\n"
     ]
    }
   ],
   "source": [
    "# 步骤 11：运行主流程\n",
    "\n",
    "# 运行主流程，使用CSV加载文档\n",
    "main(use_csv=True, file_path=\"documents.csv\", nrows=100, use_quantization=False, top_k=2)\n",
    "#main(use_csv=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
